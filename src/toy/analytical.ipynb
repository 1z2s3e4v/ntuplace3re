{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analytical Placement Toy ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0-1: Import Libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from IPython.display import display, clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0-2: Define Parameters\n",
    "GAMMA = 1.0\n",
    "LR    = 0.5\n",
    "ITER  = 50\n",
    "num_bins = 16 # grid 16x16\n",
    "target_density = 1.0\n",
    "# NTUplace parameters\n",
    "TRUNC_FACTOR = 1.0 # truncationFactor\n",
    "# RePlAce parameters\n",
    "DDW_TYPE = 2 # Dynamic Density Weight STRATEGY, 0: gWL/gD, 1: NTUplace, 2: RePlAce\n",
    "INIT_DEN_PENALTY = 0.9 # 0.00008\n",
    "HPWL_REF = 15.0\n",
    "MIN_PhiCoef = 0.95\n",
    "MAX_PhiCoef = 1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Netlist:\n",
    "    def __init__(self):\n",
    "        self.cells_name    = []\n",
    "        self.cells_size    = []\n",
    "        self.cells_pos     = [] # pos is center of cell\n",
    "        self.nets          = []\n",
    "        self.cells_isfixed = [] # 1: fixed, 0: movable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define Util Functions\n",
    "# Visualize Placements\n",
    "def plot_placement(ax, board_size, netlist, iteration, costs_info):\n",
    "    bin_size = board_size / num_bins\n",
    "    for i in range(num_bins + 1):\n",
    "        x = i * bin_size\n",
    "        y = i * bin_size\n",
    "        ax.axhline(y, color='lightgray', linewidth=0.5, zorder=0)\n",
    "        ax.axvline(x, color='lightgray', linewidth=0.5, zorder=0)\n",
    "\n",
    "    for net in netlist.nets:\n",
    "        net_cells = netlist.cells_pos[net]\n",
    "        ax.plot(net_cells[:, 0], net_cells[:, 1], 'r--', alpha=0.5)  # Draw nets\n",
    "\n",
    "    for i, (pos, size) in enumerate(zip(netlist.cells_pos, netlist.cells_size)):\n",
    "        rect = plt.Rectangle(pos - size / 2, size[0], size[1], edgecolor='blue', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(pos[0], pos[1], netlist.cells_name[i], ha='center', va='center', fontsize=8, color='blue')\n",
    "\n",
    "    ax.set_xlim(0, board_size)\n",
    "    ax.set_ylim(0, board_size)\n",
    "    # ax.set_title(f'Placement at Iteration {iteration}, WL(LSE/HPWL): {costs_info[\"total_lse\"]:.2f} / {costs_info[\"total_hpwl\"]:.2f}, Overflow(Pot/Den): {costs_info[\"total_potential_overflow\"]:.2f} / {costs_info[\"total_density_overflow\"]:.2f}, Cost(obj/real): {costs_info[\"total_obj_value\"]:.2f} / {costs_info[\"total_cost\"]:.2f}')\n",
    "    ax.set_title(f'Placement at Iteration {iteration}\\n' + \n",
    "                 f'  WL: {costs_info[\"total_lse\"]:.2f}, Overflow: {costs_info[\"total_potential_overflow\"]:.2f},    Obj_f: {costs_info[\"total_obj_value\"]:.2f}\\n' + \n",
    "                 f'  Grad: {costs_info[\"gWL\"]:.2e} + {costs_info[\"weight_density\"]:.2e} * {costs_info[\"gPot\"]:.2e} (Den Ratio: {(costs_info[\"weight_density\"]*costs_info[\"gPot\"]/costs_info[\"gWL\"]):.2e})\\n' + \n",
    "                 f'HPWL: {costs_info[\"total_hpwl\"]:.2f},  OverDen: {costs_info[\"total_density_overflow\"]:.2f}, RealCost: {costs_info[\"total_cost\"]:.2f}')\n",
    "    # ax.text(0.5, 0.92, \n",
    "    #         f'  WL: {costs_info[\"total_lse\"]:.2f}, Overflow: {costs_info[\"total_potential_overflow\"]:.2f},    Obj_f: {costs_info[\"total_obj_value\"]:.2f}\\n' + \n",
    "    #         f'HPWL: {costs_info[\"total_hpwl\"]:.2f},  OverDen: {costs_info[\"total_density_overflow\"]:.2f}, RealCost: {costs_info[\"total_cost\"]:.2f}', \n",
    "    #      ha='center', fontsize=10)\n",
    "    ax.grid(False)\n",
    "\n",
    "# Visualize Density Map\n",
    "def plot_density_map(fig, ax, board_size, density_map):\n",
    "    bin_size = board_size / num_bins\n",
    "    # Draw heat map\n",
    "    heatmap = ax.imshow(\n",
    "        density_map.T,              # transpose: make y-axis as vertical\n",
    "        origin='lower',             # let (0, 0) be at the bottom-left\n",
    "        extent=[0, board_size, 0, board_size],  # extend to chip size\n",
    "        cmap='Reds',                # heat map\n",
    "        aspect='equal'\n",
    "    )\n",
    "    cbar = fig.colorbar(heatmap, ax=ax)\n",
    "    cbar.set_label('Density')\n",
    "    for i in range(num_bins + 1):\n",
    "        x = i * bin_size\n",
    "        y = i * bin_size\n",
    "        ax.axhline(y, color='lightgray', linewidth=0.5, zorder=0)\n",
    "        ax.axvline(x, color='lightgray', linewidth=0.5, zorder=0)\n",
    "    ax.set_xlim(0, board_size)\n",
    "    ax.set_ylim(0, board_size)\n",
    "    ax.set_title(f'Density Map')\n",
    "    ax.grid(False)\n",
    "\n",
    "# Initialize Placement\n",
    "def initialize_placement(num_cells, board_size, isfixed):\n",
    "    cells_pos = np.random.rand(num_cells, 2) * board_size  # Random positions within the canvas\n",
    "    # random place movable cells at centor area (9/20 ~ 11/20)\n",
    "    for i in range(num_cells):\n",
    "        # rand float in [0.45, 0.55] * board_size\n",
    "        if isfixed[i] == 0:\n",
    "            cells_pos[i, 0] = (0.45 + 0.1 * np.random.rand()) * board_size  # x\n",
    "            cells_pos[i, 1] = (0.45 + 0.1 * np.random.rand()) * board_size  # y\n",
    "    return cells_pos\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case\n",
    "# Assume we have 5 cells and 4 nets connecting them\n",
    "board_size = 32\n",
    "netlist = Netlist()\n",
    "netlist.cells_name = ['A', 'B', 'C', 'D', 'E', 'F', 'f_1', 'f_2', 'f_3', 'f_4', 'f_5', 'f_6', 'f_7', 'f_8', 'f_9', 'f_10']\n",
    "# netlist.cells_size = np.array([[3,3], [3,3], [1,2], [2,1], [1,1], [1,1], [6,6], [6,6], [6,6], [6,6], [6,6], [6,6], [6,6], [6,6], [6,6], [6,6]])\n",
    "netlist.cells_size = np.array([[3,3], [3,3], [1,2], [2,1], [1,1], [1,1], [3,3], [3,3], [6,6], [3,3], [3,3], [6,6], [6,6], [2,2], [2,2], [6,6]])\n",
    "netlist.cells_pos = np.zeros((len(netlist.cells_name), 2))\n",
    "netlist.nets = np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "netlist.cells_isfixed = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "#nets = [[2,4,5],[0,1,3],[0,2,3,5],[0,1,2,3,4]]\n",
    "num_cells = len(netlist.cells_name)\n",
    "\n",
    "# Density Potential Initialization\n",
    "bin_size = board_size / num_bins\n",
    "bins_potential = np.zeros((num_bins, num_bins))  # bins current density (potential)\n",
    "bins_density = np.zeros((num_bins, num_bins))  # bins current density (real, only for visualization)\n",
    "bins_expect_potential = np.ones((num_bins, num_bins)) * target_density  # bins expect density (potential)\n",
    "bins_free_space = np.zeros((num_bins, num_bins)) # bins initial free space (consider fixed cells)\n",
    "bins_base_potential = np.zeros((num_bins, num_bins)) # bins initial potential (consider fixed cells)\n",
    "cells_bins_potential = [[] for _ in range(num_cells)]  # each cell's potential to contributed bins\n",
    "cells_potential_norm = np.ones(num_cells)  # each cell's potential norm\n",
    "weight_density = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSE Wirelength Model:  \n",
    "$\n",
    "W_{LSE}(\\mathbf{x}, \\mathbf{y}) = \\gamma \\sum_{e \\in E} \\left( \\ln \\sum_{v_k \\in e} \\exp(x_k / \\gamma) + \\ln \\sum_{v_k \\in e} \\exp(-x_k / \\gamma)  \n",
    "                                                                + \\ln \\sum_{v_k \\in e} \\exp(y_k / \\gamma) + \\ln \\sum_{v_k \\in e} \\exp(-y_k / \\gamma) \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wirelength as the Log-Sum-Exp HPWL \n",
    "def calculate_lse_wl(positions, nets, gamma=1.0):\n",
    "    # wl_x = r * sum( ln(sum( exp(x_k/r) )) + ln(sum( exp(-x_k/r) )) )    ### max(x) + -min(x)   ### -min(x) = max(-x)\n",
    "    total_hpwl = 0.0\n",
    "    for net in nets:\n",
    "        xs = positions[net, 0]\n",
    "        ys = positions[net, 1]\n",
    "        wl_x = gamma * ( np.log(np.sum(np.exp(xs / gamma))) + np.log(np.sum(np.exp(-xs / gamma))) )\n",
    "        wl_y = gamma * ( np.log(np.sum(np.exp(ys / gamma))) + np.log(np.sum(np.exp(-ys / gamma))) )\n",
    "        total_hpwl += wl_x + wl_y\n",
    "    return total_hpwl\n",
    "def calculate_hpwl(positions, nets):\n",
    "    # HPWL_x = max(x) - min(x)\n",
    "    hpwl = 0\n",
    "    for cell_ids in nets: # for each net\n",
    "        x_coords = positions[cell_ids, 0]\n",
    "        y_coords = positions[cell_ids, 1]\n",
    "        hpwl += (x_coords.max() - x_coords.min()) + (y_coords.max() - y_coords.min())\n",
    "    return hpwl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "We have $LSE_{x}$  \n",
    "$\n",
    "\\small\n",
    "LSE_{x} = \\log \\sum e^{x_i / \\gamma} + \\log \\sum e^{-x_i / \\gamma}\n",
    "$  \n",
    "Partial derivative with respect to $x_i$ --> we can get the gradient to optimize WL for $x_i$  \n",
    "$\n",
    "\\small\n",
    "\\frac{\\partial \\text{LSE}_{x}}{\\partial x_i} = \\left( e^{x_i / \\gamma} \\Big/ \\sum e^{x_j / \\gamma} \\right) - \\left( e^{-x_i / \\gamma} \\Big/ \\sum e^{-x_j / \\gamma} \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent to minimize objective function\n",
    "def calculate_wl_gradient(netlist, gamma=1.0):\n",
    "    cells = netlist.cells_pos; nets = netlist.nets\n",
    "    grads = np.zeros_like(cells) # gradient\n",
    "    for cell_ids in nets: # for each net\n",
    "        xs = cells[cell_ids, 0]\n",
    "        ys = cells[cell_ids, 1]\n",
    "        exp_xs = np.exp(xs / gamma)       # max(x)\n",
    "        exp_neg_xs = np.exp(-xs / gamma)  # -min(x)\n",
    "        exp_ys = np.exp(ys / gamma)       # max(y)\n",
    "        exp_neg_ys = np.exp(-ys / gamma)  # -min(y)\n",
    "\n",
    "        grad_xs = gamma * ( (exp_xs / np.sum(exp_xs)) - (exp_neg_xs / np.sum(exp_neg_xs)) ) # partial derivative for each x_i\n",
    "        grad_ys = gamma * ( (exp_ys / np.sum(exp_ys)) - (exp_neg_ys / np.sum(exp_neg_ys)) )\n",
    "\n",
    "        grads[cell_ids, 0] += grad_xs\n",
    "        grads[cell_ids, 1] += grad_ys\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bell-Shaped Potential Density Model:  \n",
    "$\n",
    "p_x(b, v) = \n",
    "\\begin{cases} \n",
    "1 - a d_x^2, & 0 \\leq d_x \\leq \\frac{w_v}{2} + w_b \\\\\n",
    "b \\left(d_x - \\frac{w_v}{2} - 2w_b \\right)^2, & \\frac{w_v}{2} + w_b \\leq d_x \\leq \\frac{w_v}{2} + 2w_b \\\\\n",
    "0, & \\frac{w_v}{2} + 2w_b \\leq d_x \n",
    "\\end{cases}\n",
    "\\quad\n",
    "\\text{, where} \\quad\n",
    "a = \\frac{4}{(w_v + 2w_b)(w_v + 4w_b)}, \\quad\n",
    "b = \\frac{2}{w_b(w_v + 4w_b)}\n",
    "$  \n",
    "\n",
    "### Gradient Descent  \n",
    "Partial derivative with respect to $x_i$ --> we can get the gradient to optimize Density for $x_i$  \n",
    "$\n",
    "\\frac{\\partial p_x(b, v)}{\\partial x_v} = \n",
    "\\begin{cases}\n",
    "-2ad_x, & 0 \\leq d_x \\leq \\frac{w_v}{2} + w_b \\\\\n",
    "2b \\left(d_x - \\frac{w_v}{2} - 2w_b \\right), & \\frac{w_v}{2} + w_b \\leq d_x \\leq \\frac{w_v}{2} + 2w_b \\\\\n",
    "0, & \\frac{w_v}{2} + 2w_b \\leq d_x\n",
    "\\end{cases}\n",
    "\\quad \\text{with} \\quad\n",
    "d_x = |x_v - x_b|\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bell-Shaped Potential \n",
    "# Refer to Eq. (6,7) and Fig. 2(b) in the paper\n",
    "# potential_x(b, v)\n",
    "def get_potential(xv, xb, cell_size, bin_size):\n",
    "    # Return the potential value of the overlap area (1-dir overlapping for one cell to one bin)\n",
    "    d = abs(xv - xb)\n",
    "    w = cell_size\n",
    "    wb = bin_size\n",
    "    if d <= w/2 + wb:\n",
    "        a = 4 / ((w + 2*wb)*(w + 4*wb))\n",
    "        return 1 - a * d**2\n",
    "    elif d <= w/2 + 2*wb:\n",
    "        b = 2 / (wb*(w + 4*wb))\n",
    "        return b * (d - w/2 - 2*wb)**2\n",
    "    else:\n",
    "        return 0.0\n",
    "# partial derivative of potential_x(b, v) w.r.t. xv\n",
    "def get_potential_gradient(xv, xb, cell_size, bin_size):\n",
    "    d = abs(xv - xb)\n",
    "    w = cell_size\n",
    "    wb = bin_size\n",
    "    if xv >= xb: # right half\n",
    "        if d <= w/2 + wb:\n",
    "            a = 4 / ((w + 2*wb)*(w + 4*wb))\n",
    "            return -2 * a * d \n",
    "        elif d <= w/2 + 2*wb:\n",
    "            b = 2 / (wb*(w + 4*wb))\n",
    "            return 2 * b * (d - w/2 - 2*wb)\n",
    "        else:\n",
    "            return 0.0\n",
    "    else: # left half\n",
    "        if d <= w/2 + wb:\n",
    "            a = 4 / ((w + 2*wb)*(w + 4*wb))\n",
    "            return 2 * a * d\n",
    "        elif d <= w/2 + 2*wb:\n",
    "            b = 2 / (wb*(w + 4*wb))\n",
    "            return -2 * b * (d - w/2 - 2*wb)\n",
    "        else:\n",
    "            return 0.0\n",
    "def get_overlap(xv, xb, cell_size, bin_size):\n",
    "    # Return the overlap dist of one cell to one bin\n",
    "    x1 = xv - cell_size / 2\n",
    "    x2 = xv + cell_size / 2\n",
    "    b1 = xb - bin_size / 2\n",
    "    b2 = xb + bin_size / 2\n",
    "    if x1 > b2 or x2 < b1:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return min(x2, b2) - max(x1, b1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contribute_bins(x, y, w, h):\n",
    "    # Find contributed bins (influence bin range < 2 bins around cell, bin_x = cell_cx +- cell_w/2 + 2*num_bins)\n",
    "    min_bin_x = max(0, int((x - (w/2 + 2*bin_size)) // bin_size))\n",
    "    max_bin_x = min(num_bins-1, int((x + (w/2 + 2*bin_size)) // bin_size))\n",
    "    min_bin_y = max(0, int((y - (h/2 + 2*bin_size)) // bin_size))\n",
    "    max_bin_y = min(num_bins-1, int((y + (h/2 + 2*bin_size)) // bin_size))\n",
    "    return min_bin_x, max_bin_x, min_bin_y, max_bin_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Potential for fixed cells\n",
    "def update_potential_base(): # UpdatePotentialGridBase()\n",
    "    global bins_free_space, bins_base_potential\n",
    "    bins_base_potential = np.zeros((num_bins, num_bins)) # initial potential = 0.0\n",
    "    bins_free_space = np.full((num_bins, num_bins), bin_size*bin_size) # initial free space = bin_size*bin_size (empty bin)\n",
    "\n",
    "    for idx, cell in enumerate(netlist.cells_pos):\n",
    "        if netlist.cells_isfixed[idx]:  # If the cell is fixed\n",
    "            x, y = cell\n",
    "            w, h = netlist.cells_size[idx]\n",
    "            min_bin_x, max_bin_x, min_bin_y, max_bin_y = find_contribute_bins(x, y, w, h)\n",
    "            # bx, by are bin index (not position)\n",
    "            for bx in range(min_bin_x, max_bin_x+1):\n",
    "                for by in range(min_bin_y, max_bin_y+1):\n",
    "                    bin_cx = bx * bin_size + bin_size/2\n",
    "                    bin_cy = by * bin_size + bin_size/2\n",
    "                    potential_x = get_potential(x, bin_cx, w, bin_size)\n",
    "                    potential_y = get_potential(y, bin_cy, h, bin_size)\n",
    "                    potential = potential_x * potential_y * w * h \n",
    "                    overlap_x = get_overlap(x, bin_cx, w, bin_size)\n",
    "                    overlap_y = get_overlap(y, bin_cy, w, bin_size)\n",
    "                    overlap = overlap_x * overlap_y\n",
    "                    bins_free_space[bx, by] -= overlap\n",
    "                    bins_base_potential[bx, by] += potential\n",
    "    return\n",
    "\n",
    "def level_smooth_base_potential(basePotential, delta): # LevelSmoothBasePotential\n",
    "    oldPotential = basePotential.copy()\n",
    "    maxPotential = np.max(oldPotential)\n",
    "    totalPotential = np.sum(oldPotential)\n",
    "    avgPotential = totalPotential / oldPotential.size\n",
    "    if totalPotential == 0:\n",
    "        return  # 無 preplaced block\n",
    "    # Apply TSP-style smoothing\n",
    "    newPotential = np.zeros_like(oldPotential)\n",
    "    for i in range(basePotential.shape[0]):\n",
    "        for j in range(basePotential.shape[1]):\n",
    "            val = oldPotential[i][j]\n",
    "            if val >= avgPotential:\n",
    "                newPotential[i][j] = avgPotential + ((val - avgPotential) / maxPotential) ** delta * maxPotential\n",
    "            else:\n",
    "                newPotential[i][j] = avgPotential - ((avgPotential - val) / maxPotential) ** delta * maxPotential\n",
    "    # Normalize total potential to match original\n",
    "    newTotal = np.sum(newPotential)\n",
    "    ratio = totalPotential / newTotal if newTotal != 0 else 1.0\n",
    "    basePotential[:, :] = newPotential * ratio\n",
    "# smooth_base_potential(smooth_r=5, smooth_delta=1.0)\n",
    "def smooth_base_potential(smooth_r=1.0, smooth_delta=1.0): # SmoothBasePotential()\n",
    "    global bins_base_potential\n",
    "    # Step 1: Gaussian smoothing\n",
    "    smoothed = gaussian_filter(bins_base_potential, sigma=smooth_r)\n",
    "    bins_base_potential[:, :] = smoothed\n",
    "    # Step 2: Level smoothing (only for multi-level, smooth_delta > 1)\n",
    "    if smooth_delta > 1.001:\n",
    "        level_smooth_base_potential(bins_base_potential, smooth_delta) # pass by reference\n",
    "        return\n",
    "    # Step 3: Additional height boost for fully blocked bins (Find the bins that are fully blocked but being too much smoothed by Gaussian)\n",
    "    more_smooth = gaussian_filter(bins_base_potential, sigma=smooth_r * 6) # more smoothed version for boost\n",
    "    bin_area = bin_size * bin_size\n",
    "    half_bin_area = bin_area / 2\n",
    "    scale = 3\n",
    "    for i in range(bins_base_potential.shape[0]):\n",
    "        for j in range(bins_base_potential.shape[1]):\n",
    "            free = bin_area - bins_base_potential[i][j]\n",
    "            # If the bin's potential is already no space or high enough, boost its potential\n",
    "            if free < 1e-4 and more_smooth[i][j] > half_bin_area:\n",
    "                bins_base_potential[i][j] += (more_smooth[i][j] - half_bin_area) * scale\n",
    "\n",
    "def update_exp_bin_potential(target_util=1.0, show_info=False): # UpdateExpBinPotential\n",
    "    global bins_base_potential, bins_expect_potential\n",
    "    bins_expect_potential = np.zeros_like(bins_base_potential)\n",
    "    total_free = 0\n",
    "    zero_bin = 0\n",
    "    for i in range(num_bins):\n",
    "        for j in range(num_bins):\n",
    "            base = bins_base_potential[i, j]\n",
    "            # Calculate the free space available in the bin\n",
    "            free = bin_size * bin_size - base  # Simplified for illustration\n",
    "            if( free > 1e-4 ):\n",
    "                bins_expect_potential[i, j] = free * target_util\n",
    "                total_free += bins_expect_potential[i, j]\n",
    "            else:\n",
    "                zero_bin += 1\n",
    "    \n",
    "    totalMovableModuleArea = np.sum(netlist.cells_size[netlist.cells_isfixed == 0][:, 0] * netlist.cells_size[netlist.cells_isfixed == 0][:, 1])\n",
    "    algUtil = totalMovableModuleArea / total_free\n",
    "    if show_info:\n",
    "        print(f\"PBIN: Zero space bin #= {zero_bin} ({100*zero_bin/num_bins/num_bins}%).  Algorithm utilization= {algUtil}\")\n",
    "# Init potential\n",
    "def init_density_model():\n",
    "    global bins_potential, bins_density, bins_expect_potential, bins_free_space, bins_base_potential, cells_bins_potential, cells_potential_norm, weight_density, MIN_PhiCoef, MAX_PhiCoef\n",
    "    bins_potential = np.zeros((num_bins, num_bins))  # bins current density (potential)\n",
    "    bins_density = np.zeros((num_bins, num_bins))  # bins current density (real, only for visualization)\n",
    "    bins_expect_potential = np.ones((num_bins, num_bins)) * target_density  # bins expect density (potential)\n",
    "    bins_free_space = np.zeros((num_bins, num_bins)) # bins initial free space (consider fixed cells)\n",
    "    bins_base_potential = np.zeros((num_bins, num_bins)) # bins initial potential (consider fixed cells)\n",
    "    cells_bins_potential = [[] for _ in range(num_cells)]  # each cell's potential to contributed bins\n",
    "    cells_potential_norm = np.ones(num_cells)  # each cell's potential norm\n",
    "    weight_density = 0.0\n",
    "    MIN_PhiCoef = 0.95\n",
    "    MAX_PhiCoef = 1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute New Potential for Bins\n",
    "def compute_new_potential_grid():\n",
    "    global cells_bins_potential, cells_potential_norm\n",
    "    cells_bins_potential = [[] for _ in range(num_cells)]\n",
    "    cells_potential_norm = np.ones(num_cells)\n",
    "    for i, (x, y) in enumerate(netlist.cells_pos):\n",
    "        if netlist.cells_isfixed[i]:  # If the cell is fixed\n",
    "            continue\n",
    "        w, h = netlist.cells_size[i]\n",
    "        total_potential = 0\n",
    "        # Find contributed bins (influence bin range < 2 bins around cell, bin_x = cell_cx +- cell_w/2 + 2*num_bins)\n",
    "        min_bin_x, max_bin_x, min_bin_y, max_bin_y = find_contribute_bins(x, y, w, h)\n",
    "        # bx, by are bin index (not position)\n",
    "        for bx in range(min_bin_x, max_bin_x+1):\n",
    "            for by in range(min_bin_y, max_bin_y+1):\n",
    "                bin_cx = bx * bin_size + bin_size/2\n",
    "                bin_cy = by * bin_size + bin_size/2\n",
    "                potential_x = get_potential(x, bin_cx, w, bin_size)\n",
    "                potential_y = get_potential(y, bin_cy, h, bin_size)\n",
    "                potential = potential_x * potential_y * w * h  # 乘上 cell 面積\n",
    "                if potential > 0:\n",
    "                    cells_bins_potential[i].append((bx, by, potential))\n",
    "                    total_potential += potential\n",
    "        cells_potential_norm[i] = netlist.cells_size[i][0] * netlist.cells_size[i][1] / total_potential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Potential for Bins\n",
    "def update_potential_grid():\n",
    "    global bins_potential\n",
    "    bins_potential[:, :] = 0 # np.zeros((num_bins, num_bins))\n",
    "    for i in range(num_cells):\n",
    "        for bx, by, pot in cells_bins_potential[i]:\n",
    "            bins_potential[bx, by] += pot * cells_potential_norm[i]\n",
    "    # only for visualization, check golden density\n",
    "    global bins_density\n",
    "    bins_density[:, :] = 0\n",
    "    bins_used_area = np.zeros((num_bins, num_bins))\n",
    "    for i in range(num_cells):\n",
    "        for bx, by, pot in cells_bins_potential[i]:\n",
    "            x1, x2, y1, y2 = netlist.cells_pos[i][0] - netlist.cells_size[i][0]/2, netlist.cells_pos[i][0] + netlist.cells_size[i][0]/2, netlist.cells_pos[i][1] - netlist.cells_size[i][1]/2, netlist.cells_pos[i][1] + netlist.cells_size[i][1]/2\n",
    "            bx1, bx2, by1, by2 = bx * bin_size, (bx+1) * bin_size, by * bin_size, (by+1) * bin_size\n",
    "            overlap_area = max(0, min(x2, bx2) - max(x1, bx1)) * max(0, min(y2, by2) - max(y1, by1))\n",
    "            bins_used_area[bx, by] += overlap_area\n",
    "    bins_density = bins_used_area / (bin_size**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Potential Gradient\n",
    "def calculate_density_gradient():\n",
    "    cells = netlist.cells_pos\n",
    "    grads = np.zeros_like(cells) # gradient\n",
    "    for i in range(num_cells):\n",
    "        grad_x = grad_y = 0\n",
    "        for bx, by, pot in cells_bins_potential[i]:\n",
    "            bin_cx = bx * bin_size + bin_size/2\n",
    "            bin_cy = by * bin_size + bin_size/2\n",
    "            gx = get_potential_gradient(cells[i, 0], bin_cx, netlist.cells_size[i][0], bin_size)\n",
    "            gy = get_potential_gradient(cells[i, 1], bin_cy, netlist.cells_size[i][1], bin_size)\n",
    "            diff = bins_potential[bx, by] - bins_expect_potential[bx, by] # loss (distance between target)\n",
    "\n",
    "            grad_x += gx * diff\n",
    "            grad_y += gy * diff\n",
    "        grads[i, 0] = grad_x\n",
    "        grads[i, 1] = grad_y\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate overflow potential\n",
    "def calculate_overflow_potential():\n",
    "    overflow = np.maximum(bins_potential - bins_expect_potential, 0)\n",
    "    return np.sum(overflow), overflow, np.max(overflow)\n",
    "def calculate_overflow_density():\n",
    "    overflow = np.maximum(bins_density - target_density, 0)\n",
    "    return np.sum(overflow), overflow, np.max(overflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function  \n",
    "$\n",
    "\\min \\; W(\\mathbf{x}, \\mathbf{y}) + 0.5\\lambda \\sum_b \\left( \\hat{D}_b(\\mathbf{x}, \\mathbf{y}) - M_b \\right)^2\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_force(gradients):\n",
    "    # grad truncation with expMaxGradSquare=(avgGrad * param.truncationFactor)^2\n",
    "    cells = netlist.cells_pos\n",
    "    totalGrad = 0\n",
    "    for i in range(num_cells):\n",
    "        totalGrad += gradients[i, 0] * gradients[i, 0] + gradients[i, 1] * gradients[i, 1]\n",
    "    avgGrad = math.sqrt( totalGrad / num_cells )\n",
    "    expMaxGrad = avgGrad * TRUNC_FACTOR # x + y\n",
    "    expMaxGradSquare = expMaxGrad * expMaxGrad\n",
    "\n",
    "    for i in range(num_cells):\n",
    "        gradSquare = gradients[i, 0] * gradients[i, 0] + gradients[i, 1] * gradients[i, 1]\n",
    "        if gradSquare > expMaxGradSquare:\n",
    "            scale = expMaxGrad / math.sqrt(gradSquare)\n",
    "            gradients[i, 0] *= scale\n",
    "            gradients[i, 1] *= scale\n",
    "    return gradients\n",
    "def get_phi_coef(scaled_diff_hpwl):\n",
    "    new_coef = MAX_PhiCoef if (scaled_diff_hpwl < 0) else MAX_PhiCoef * pow( MAX_PhiCoef, scaled_diff_hpwl * -1.0 )\n",
    "    new_coef = max(MIN_PhiCoef, new_coef)\n",
    "    return new_coef\n",
    "\n",
    "def calculate_objective_gradient(lambda_prev=None, hpwl=0, hpwl_prev=0):\n",
    "    cells = netlist.cells_pos\n",
    "    grads = np.zeros_like(cells) # gradient\n",
    "    grad_pot = calculate_density_gradient() # get potential gradients\n",
    "    grad_wl = calculate_wl_gradient(netlist) # get wirelength gradients\n",
    "    # Determine the new density_weight (lambda)\n",
    "    weight_density = 0.0\n",
    "    if DDW_TYPE == 0: # Just use gWL/gD\n",
    "        weight_density = np.sum(np.abs(grad_wl)) / np.sum(np.abs(grad_pot))\n",
    "    elif DDW_TYPE == 1: # NTUplace: density_weight (lambda) is initialized according to the strength of wirelength and density gradients, and is increased by two times for each iteration\n",
    "        if lambda_prev is None:\n",
    "            weight_density = np.sum(np.abs(grad_wl)) / np.sum(np.abs(grad_pot))\n",
    "        else:\n",
    "            weight_density = lambda_prev * 2\n",
    "    elif DDW_TYPE == 2: # RePlAce:\n",
    "        if lambda_prev is None:\n",
    "            weight_density = np.sum(np.abs(grad_wl)) / np.sum(np.abs(grad_pot)) * INIT_DEN_PENALTY\n",
    "        else:\n",
    "            phi_coef = get_phi_coef( (hpwl - hpwl_prev) / HPWL_REF )\n",
    "            weight_density = lambda_prev * phi_coef\n",
    "    grads = grad_wl + 0.5 * grad_pot * weight_density  # combine gradients\n",
    "\n",
    "    # AdjustForce(): truncation grad with expMaxGradSquare=(avgGrad * param.truncationFactor)^2\n",
    "    grads = adjust_force(grads)\n",
    "\n",
    "    return grads, weight_density, grad_wl, grad_pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate objective value and real cost\n",
    "def calculate_obj_value(total_wl, total_potential, _lambda):\n",
    "    obj_value = total_wl + 0.5 * _lambda * total_potential\n",
    "    return obj_value\n",
    "def calculate_cost(total_hpwl, total_density, _lambda):\n",
    "    cost = total_hpwl + 0.5 * _lambda * total_density\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update cell positions using gradient descent  \n",
    "$\n",
    "x_k = x_{k-1} - stepSize * gradient\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update cell positions using gradient descent\n",
    "def update_positions(cells_pos, gradient, learning_rate=0.1, is_fixed=None):\n",
    "    if is_fixed is None:\n",
    "        is_fixed = np.zeros(len(cells_pos), dtype=bool)\n",
    "    is_fixed = is_fixed.astype(bool)\n",
    "    movable_mask = ~is_fixed[:, None]  # shape: (N, 1)\n",
    "    cells_pos -= learning_rate * gradient * movable_mask\n",
    "    return cells_pos\n",
    "\n",
    "# Ensure cells stay within the board boundaries\n",
    "def bound_cells(cells_pos):\n",
    "    # pos = boundary +- cell_size / 2\n",
    "    cells_pos[:, 0] = np.clip(cells_pos[:, 0], netlist.cells_size[:, 0] / 2, board_size - netlist.cells_size[:, 0] / 2)\n",
    "    cells_pos[:, 1] = np.clip(cells_pos[:, 1], netlist.cells_size[:, 1] / 2, board_size - netlist.cells_size[:, 1] / 2)\n",
    "    return cells_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random positions for cells (2D positions: x, y)\n",
    "np.random.seed(666)  # For reproducibility\n",
    "init_pos = initialize_placement(len(netlist.cells_name), board_size, netlist.cells_isfixed)\n",
    "init_pos = bound_cells(init_pos)\n",
    "netlist.cells_pos = init_pos.copy()\n",
    "# Initialize bins expected potential\n",
    "init_density_model()\n",
    "update_potential_base()\n",
    "smooth_base_potential()\n",
    "update_exp_bin_potential(show_info=True)\n",
    "# Visualization Check \n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "costs_info = {\"total_hpwl\": 123, \"total_lse\": 123, \"total_density_overflow\": 123, \"total_potential_overflow\": 123, \"total_obj_value\": 123, \"total_cost\": 123, \"weight_density\": weight_density, \"gWL\": 123, \"gPot\": 123}\n",
    "plot_placement(axs[0], board_size, netlist, iter, costs_info)\n",
    "plot_density_map(fig, axs[1], board_size, bins_base_potential) # plot_density_map(axs[1], board_size, bins_density)\n",
    "# display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main analytical placement function ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main analytical placement function ## \n",
    "# Initialize cells positions and parameters\n",
    "netlist.cells_pos = init_pos.copy()\n",
    "num_iter = ITER\n",
    "gamma = GAMMA\n",
    "step_size = LR\n",
    "weight_density = None\n",
    "DDW_TYPE = 0 # Dynamic Density Weight STRATEGY, 0: gWL/gD, 1: NTUplace, 2: RePlAce\n",
    "\n",
    "early_stop = False\n",
    "enough_iter = 15\n",
    "total_over_den_prev = 0\n",
    "max_den_prev = 0\n",
    "# Iteratively optimize placement\n",
    "for iter in range(num_iter):\n",
    "    # Calculate wirelength\n",
    "    total_hpwl = calculate_hpwl(netlist.cells_pos, netlist.nets)\n",
    "    total_lse = calculate_lse_wl(netlist.cells_pos, netlist.nets, gamma=gamma)\n",
    "    # Calculate Density\n",
    "    compute_new_potential_grid()\n",
    "    update_potential_grid()\n",
    "    total_density_overflow, density_overflow_map, max_den = calculate_overflow_density()\n",
    "    total_potential_overflow, potential_overflow_map, max_pot = calculate_overflow_potential()\n",
    "\n",
    "    # Calculate gradient\n",
    "    gradient, lambda_new, gWL, gPot = calculate_objective_gradient(lambda_prev=weight_density) # actually here is calculate_potentail_gradient\n",
    "    weight_density = lambda_new\n",
    "    # Update positions\n",
    "    netlist.cells_pos = update_positions(netlist.cells_pos, gradient, learning_rate=step_size, is_fixed=netlist.cells_isfixed)\n",
    "    netlist.cells_pos = bound_cells(netlist.cells_pos) # bound cells to board size\n",
    "    # Calculate objective value\n",
    "    total_obj_value = calculate_obj_value(total_lse, total_potential_overflow, weight_density)\n",
    "    total_cost = calculate_cost(total_hpwl, total_density_overflow, weight_density)\n",
    "\n",
    "    # Early stop\n",
    "    enoughIter = iter > enough_iter\n",
    "    spreadEnough = total_density_overflow < target_density + 0.2\n",
    "    increaseOverDen = total_density_overflow > total_over_den_prev\n",
    "    increaseMaxDen = max_den > max_den_prev\n",
    "    notEfficientOptimize = 0.5 * total_potential_overflow * weight_density / total_obj_value * 100.0 > 95\n",
    "    # if enoughIter and np.linalg.norm(gradient) > np.linalg.norm(g_prev):\n",
    "    #     # print(f'Converged at iteration {iter}')\n",
    "    #     early_stop = True\n",
    "    if enoughIter and notEfficientOptimize:\n",
    "        print(f'Failed to further optimize at iteration {iter}')\n",
    "        early_stop = True\n",
    "    if enoughIter and spreadEnough and increaseOverDen and increaseMaxDen:\n",
    "        print(f'Cannot further reduce over density at iteration {iter}')\n",
    "        early_stop = True\n",
    "    # update last values\n",
    "    total_over_den_prev = total_density_overflow\n",
    "    max_den_prev = max_den\n",
    "\n",
    "\n",
    "    # Plot placement\n",
    "    costs_info = {\"total_hpwl\": total_hpwl, \"total_lse\": total_lse, \"total_density_overflow\": total_density_overflow, \"total_potential_overflow\": total_potential_overflow, \"total_obj_value\": total_obj_value, \"total_cost\": total_cost, \"weight_density\": weight_density, \"gWL\": np.sum(np.abs(gWL)), \"gPot\": np.sum(np.abs(gPot))}\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    plot_placement(axs[0], board_size, netlist, iter, costs_info)\n",
    "    plot_density_map(fig, axs[1], board_size, bins_density)\n",
    "    display(fig)\n",
    "    clear_output(wait=True)\n",
    "    plt.close(fig)\n",
    "    plt.pause(0.01)\n",
    "\n",
    "    if early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Gradient Descent: CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.array([[0, 0], [0, 0], [0, 0]])\n",
    "grad = np.array([[-3, -0.2], [1, 1], [-0.08, -0.1]])\n",
    "g_prev = np.array([[-1, -1], [0.2, 3], [-0.2, -0.08]])\n",
    "d_prev = np.array([[1.2, 1.2], [1.2, 1.2], [20, -4]])\n",
    "# beta = compute_beta_polak_ribiere_xy(grad, g_prev)\n",
    "beta = grad * (grad - g_prev) / np.sum(g_prev**2, axis=0)\n",
    "dir = -grad + beta * d_prev\n",
    "# alpha = step_size / (np.linalg.norm(dir, axis=1, keepdims=True) + 1e-10)  # shape: (N, 2)\n",
    "alpha = step_size / np.linalg.norm(dir, axis=0)\n",
    "update = alpha * dir\n",
    "print(f\"beta: {beta}\")\n",
    "print(f\"dir: {dir}\")\n",
    "print(f\"alpha: {alpha}\")\n",
    "print(f\"update: {update}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_hist= {\"pr\": [], \"rl\": [], \"combined\": []}; beta_hist= {\"pr\": [], \"rl\": [], \"combined\": []}; lambda_hist= {\"pr\": [], \"rl\": [], \"combined\": []}\n",
    "def save_alpha_beta_lambda(_alpha, _beta, _lambda, _type):\n",
    "    if _type == \"pr\":\n",
    "        alpha_hist[\"pr\"].append(_alpha)\n",
    "        beta_hist[\"pr\"].append(_beta)\n",
    "        lambda_hist[\"pr\"].append(_lambda)\n",
    "    elif _type == \"rl\":\n",
    "        alpha_hist[\"rl\"].append(_alpha)\n",
    "        beta_hist[\"rl\"].append(_beta)\n",
    "        lambda_hist[\"rl\"].append(_lambda)\n",
    "    elif _type == \"combined\":\n",
    "        alpha_hist[\"combined\"].append(_alpha)\n",
    "        beta_hist[\"combined\"].append(_beta)\n",
    "        lambda_hist[\"combined\"].append(_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CG (conjugate gradient) with Dynemic Step Size\n",
    "# Compute β_k (Polak-Ribiere formula) with scaling to avoid overflow\n",
    "def compute_beta_polak_ribiere_xy(grad, g_prev):\n",
    "    \"\"\"\n",
    "    grad, g_prev: shape (N, 2)\n",
    "    returns: beta: shape (N, 2)，對 x/y 各自計算 β\n",
    "    \"\"\"\n",
    "    eps = 1e-10\n",
    "    # Compute maximum absolute value for scaling to avoid overflow\n",
    "    max_grad = np.maximum(np.max(np.abs(grad), axis=0), np.max(np.abs(g_prev), axis=0))\n",
    "    max_grad = np.where(max_grad < eps, eps, max_grad)\n",
    "    # Normalize the gradients\n",
    "    grad_scaled = grad / max_grad\n",
    "    g_prev_scaled = g_prev / max_grad\n",
    "    # Numerator: g_k^T (g_k - g_{k-1})\n",
    "    diff = grad_scaled - g_prev_scaled\n",
    "    numerator = grad_scaled * diff\n",
    "    # Denominator: g_{k-1}^T g_{k-1}\n",
    "    denominator = g_prev_scaled ** 2 + eps\n",
    "    denominator = np.where(denominator < eps, eps, denominator)  # avoid divide by 0\n",
    "    # Beta: Polak-Ribiere formula\n",
    "    beta = numerator / denominator  # element-wise (N,2)\n",
    "    return beta\n",
    "# Update cell positions\n",
    "def update_positions_cg(cells_pos, g_prev, d_prev, grad, step_size=1.0, is_fixed=None):\n",
    "    # (0) set movable mask\n",
    "    if is_fixed is None:\n",
    "        is_fixed = np.zeros(len(cells_pos), dtype=bool)\n",
    "    is_fixed = is_fixed.astype(bool)\n",
    "    movable_mask = ~is_fixed[:, None]  # shape: (N, 1)\n",
    "\n",
    "    # (1) We have gradient directions grad = g_k = ∇f(x_k)\n",
    "    # (2) Compute Polak-Ribiere parameter β_k (accelerate (sum) if gradient become better. slow down (subtract) if gradient become slower; go back (subtract very much) if gradient become bad)\n",
    "    # beta = grad * (grad - g_prev) / np.sum(g_prev**2, axis=0)\n",
    "    beta = compute_beta_polak_ribiere_xy(grad, g_prev)\n",
    "    # beta = np.clip(beta, 0, None)  # make sure β_k >= 0\n",
    "    # (3) Compute conjugate directions d = -grad + beta*d_prev\n",
    "    dir = -grad + beta * d_prev # d_prev.shape = (N, 2)\n",
    "    # (4) Compute step size alpha = s/norm(d)\n",
    "    alpha = step_size / np.linalg.norm(dir, axis=0)\n",
    "    # (5) update positions: x = x_prev + alpha*d\n",
    "    cells_pos += (alpha * dir) * movable_mask\n",
    "\n",
    "    # Test: save alpha, beta, lambda for visualization\n",
    "    save_alpha_beta_lambda(alpha, beta, np.full((gWL.shape[0], 1), weight_density), \"pr\")\n",
    "    return cells_pos, dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main analytical placement function ## \n",
    "# Initialize cells positions and parameters\n",
    "netlist.cells_pos = init_pos.copy()\n",
    "num_iter = ITER\n",
    "gamma = GAMMA\n",
    "step_size = board_size / 32\n",
    "weight_density = None\n",
    "DDW_TYPE = 2 # Dynamic Density Weight STRATEGY, 0: gWL/gD, 1: NTUplace, 2: RePlAce\n",
    "\n",
    "g_prev = np.zeros_like(netlist.cells_pos)  # init grad = 0\n",
    "d_prev = np.zeros_like(netlist.cells_pos)  # init dir = 0\n",
    "hpwl_prev = 0\n",
    "early_stop = False\n",
    "enough_iter = 15\n",
    "total_over_den_prev = 0\n",
    "max_den_prev = 0\n",
    "max_backtrak = 4 # 10\n",
    "isMaxPhiCoefChanged = False\n",
    "# Iteratively optimize placement\n",
    "for iter in range(num_iter):\n",
    "    # Calculate wirelength\n",
    "    total_hpwl = calculate_hpwl(netlist.cells_pos, netlist.nets)\n",
    "    total_lse = calculate_lse_wl(netlist.cells_pos, netlist.nets, gamma=gamma)\n",
    "    # Calculate Density\n",
    "    compute_new_potential_grid()\n",
    "    update_potential_grid()\n",
    "    total_density_overflow, density_overflow_map, max_den = calculate_overflow_density()\n",
    "    total_potential_overflow, potential_overflow_map, max_pot = calculate_overflow_potential()\n",
    "\n",
    "    # Calculate gradient\n",
    "    gradient, lambda_new, gWL, gPot = calculate_objective_gradient(lambda_prev=weight_density, hpwl=total_hpwl, hpwl_prev=hpwl_prev) # actually here is calculate_potentail_gradient\n",
    "    weight_density = lambda_new\n",
    "    # Update positions\n",
    "    netlist.cells_pos, dk = update_positions_cg(netlist.cells_pos, g_prev, d_prev, gradient, step_size=step_size, is_fixed=netlist.cells_isfixed)\n",
    "    netlist.cells_pos = bound_cells(netlist.cells_pos) # bound cells to board size\n",
    "    # Calculate objective value\n",
    "    total_obj_value = calculate_obj_value(total_lse, total_potential_overflow, weight_density)\n",
    "    total_cost = calculate_cost(total_hpwl, total_density_overflow, weight_density)\n",
    "\n",
    "    # Check early stop\n",
    "    enoughIter = iter > enough_iter\n",
    "    spreadEnough = total_density_overflow < target_density + 0.2\n",
    "    increaseOverDen = total_potential_overflow > total_over_den_prev # total_density_overflow > total_over_den_prev\n",
    "    increaseMaxDen = max_den > max_den_prev\n",
    "    notEfficientOptimize = 0.5 * total_potential_overflow * weight_density / total_obj_value * 100.0 > 95\n",
    "    if enoughIter and notEfficientOptimize:\n",
    "        print(f'Failed to further optimize at iteration {iter}')\n",
    "        early_stop = True\n",
    "    if enoughIter and spreadEnough and increaseOverDen and increaseMaxDen:\n",
    "        print(f'Cannot further reduce over density at iteration {iter}')\n",
    "        early_stop = True\n",
    "    # update last values\n",
    "    g_prev = gradient.copy(); d_prev = dk.copy()\n",
    "    total_over_den_prev = total_potential_overflow #total_density_overflow\n",
    "    max_den_prev = max_den\n",
    "    hpwl_prev = total_hpwl\n",
    "    if isMaxPhiCoefChanged and ((iter) % max_backtrak) == 0:\n",
    "        isMaxPhiCoefChanged = False\n",
    "    if isMaxPhiCoefChanged and ((iter+1) % max_backtrak) == 0:\n",
    "        isMaxPhiCoefChanged = True\n",
    "        MAX_PhiCoef *= 0.99\n",
    "    \n",
    "\n",
    "    # Plot placement\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5), facecolor='white')\n",
    "    costs_info = {\"total_hpwl\": total_hpwl, \"total_lse\": total_lse, \"total_density_overflow\": total_density_overflow, \"total_potential_overflow\": total_potential_overflow, \"total_obj_value\": total_obj_value, \"total_cost\": total_cost, \"weight_density\": weight_density, \"gWL\": np.sum(np.abs(gWL)), \"gPot\": np.sum(np.abs(gPot))}\n",
    "    plot_placement(axs[0], board_size, netlist, iter, costs_info)\n",
    "    plot_density_map(fig, axs[1], board_size, bins_potential) # plot_density_map(axs[1], board_size, bins_density)\n",
    "    display(fig)\n",
    "    clear_output(wait=True)\n",
    "    plt.close(fig)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    if early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== plot α, β, λ trend（前 6 個 cell） ==========\n",
    "fig, axs = plt.subplots(3, 6, figsize=(18, 6))\n",
    "keys = [\"Polak-Ribiere\", \"RL\", \"Combined\"]\n",
    "for i in range(6):\n",
    "    axs[0][i].plot([v[i] for v in alpha_hist[\"pr\"]], label=keys[0])\n",
    "    axs[0][i].set_title(f\"Cell {i}\")\n",
    "    axs[0][i].set_ylabel(\"α\")\n",
    "    axs[0][i].legend()\n",
    "\n",
    "    axs[1][i].plot([v[i] for v in beta_hist[\"pr\"]], label=keys[0])\n",
    "    axs[1][i].set_ylabel(\"β\")\n",
    "    axs[1][i].legend()\n",
    "\n",
    "    axs[2][i].plot([v[i] for v in lambda_hist[\"pr\"]], label=keys[0])\n",
    "    axs[2][i].set_ylabel(\"λ\")\n",
    "    axs[2][i].legend()\n",
    "\n",
    "for i in range(6):\n",
    "    axs[2][i].set_xlabel(\"Iteration\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-Based Gradient Descent for update_positions_cg(): Determine ($\\alpha$, $\\beta$, $\\lambda$) For Each Cell with RL\n",
    "1. Construct RL model\n",
    "2. Train 100 times with different initial placemnet with the same toy case\n",
    "3. Goal: minimized objective function (WL + wD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO-based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct RL Model\n",
    "# Use PPO model to determine each cell's alpha, beta, lambda\n",
    "# Model input: This cell's current and last iter (gWL, gPot, λ, α, β, λ)\n",
    "# Model output: (α, β, λ) (For this iter)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "# PPO Policy Network (per-cell)\n",
    "class RLPolicy(nn.Module):\n",
    "    def __init__(self, input_dim=9, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.alpha_head = nn.Linear(hidden_dim, 1)\n",
    "        self.beta_head  = nn.Linear(hidden_dim, 1)\n",
    "        self.lambda_head= nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        alpha  = F.softplus(self.alpha_head(x)) + 1e-3  # ensure > 0\n",
    "        beta   = F.softplus(self.beta_head(x))\n",
    "        lambd  = F.softplus(self.lambda_head(x)) + 1e-3\n",
    "        \n",
    "        # log probs of normal distributions (optional for entropy/ppo loss)\n",
    "        log_alpha = torch.log(alpha)\n",
    "        log_beta = torch.log(beta + 1e-6)\n",
    "        log_lambda = torch.log(lambd)\n",
    "        # composite_logprob = log_alpha + log_beta + log_lambda   \n",
    "        composite_logprob = torch.log(alpha + 1e-6) + torch.log(beta + 1e-6) + torch.log(lambd + 1e-6)\n",
    "\n",
    "        return alpha, beta, lambd, composite_logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replay Buffer\n",
    "class PPOBuffer:\n",
    "    def __init__(self, max_size=5000):\n",
    "        self.state = deque(maxlen=max_size)\n",
    "        self.actions = deque(maxlen=max_size)\n",
    "        self.rewards = deque(maxlen=max_size)\n",
    "        self.logprobs = deque(maxlen=max_size)\n",
    "\n",
    "    def store(self, state, action, reward, logprob):\n",
    "        self.state.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.logprobs.append(logprob)\n",
    "\n",
    "    def get(self):\n",
    "        return (torch.tensor(np.array(self.state), dtype=torch.float32),\n",
    "                torch.tensor(np.array(self.actions), dtype=torch.float32),\n",
    "                torch.tensor(np.array(self.rewards), dtype=torch.float32),\n",
    "                torch.tensor(np.array(self.logprobs), dtype=torch.float32))\n",
    "\n",
    "    def clear(self):\n",
    "        self.state.clear()\n",
    "        self.actions.clear()\n",
    "        self.rewards.clear()\n",
    "        self.logprobs.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reward Function (objective value improvement)\n",
    "def compute_reward(obj_prev, obj_now):\n",
    "    return obj_prev - obj_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Process for PPO policy (advantage = reward baseline)\n",
    "def train_ppo(ppo_policy, ppo_optimizer, buffer, gamma=0.99, clip_ratio=0.2, ent_coeff=0.01):\n",
    "    state_batch, act_batch, rew_batch, logp_old_batch = buffer.get()\n",
    "    # normalize rewards (optional)\n",
    "    rew_batch = (rew_batch - rew_batch.mean()) / (rew_batch.std() + 1e-8)\n",
    "    # forward pass\n",
    "    alpha, beta, lambd, logp = ppo_policy(state_batch)\n",
    "    act_pred = torch.cat([alpha, beta, lambd], dim=1)\n",
    "    # surrogate loss with simple squared advantage (no GAE)\n",
    "    adv = rew_batch\n",
    "    loss_pi = ((act_pred - act_batch)**2).mean() - ent_coeff * logp.mean()\n",
    "    ppo_optimizer.zero_grad()\n",
    "    loss_pi.backward()\n",
    "    ppo_optimizer.step()\n",
    "\n",
    "    return loss_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Collect Transition (state, action, reward, logprob) \n",
    "def collect_transition(buffer, obs_scalar, action, reward, logprob):\n",
    "    for i in range(len(obs_scalar)):\n",
    "        buffer.store(obs_scalar[i], action[i], reward[i], logprob[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualizing alpha, beta, lambda\n",
    "alpha_hist= {\"pr\": [], \"rl\": [], \"combined\": []}; beta_hist= {\"pr\": [], \"rl\": [], \"combined\": []}; lambda_hist= {\"pr\": [], \"rl\": [], \"combined\": []}\n",
    "def save_alpha_beta_lambda(_alpha, _beta, _lambda, _type):\n",
    "    if _type == \"pr\":\n",
    "        alpha_hist[\"pr\"].append(_alpha)\n",
    "        beta_hist[\"pr\"].append(_beta)\n",
    "        lambda_hist[\"pr\"].append(_lambda)\n",
    "    elif _type == \"rl\":\n",
    "        alpha_hist[\"rl\"].append(_alpha)\n",
    "        beta_hist[\"rl\"].append(_beta)\n",
    "        lambda_hist[\"rl\"].append(_lambda)\n",
    "    elif _type == \"combined\":\n",
    "        alpha_hist[\"combined\"].append(_alpha)\n",
    "        beta_hist[\"combined\"].append(_beta)\n",
    "        lambda_hist[\"combined\"].append(_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL-assisted CG (conjugate gradient) with Dynemic Step Size\n",
    "# Update cell positions\n",
    "def update_positions_rlcg(cells_pos, g_prev, d_prev, grad, ppo_policy, obs, step_size=1.0, is_fixed=None):\n",
    "    # (0) Set movable mask\n",
    "    if is_fixed is None:\n",
    "        is_fixed = np.zeros(len(cells_pos), dtype=bool)\n",
    "    is_fixed = is_fixed.astype(bool)\n",
    "    movable_mask = ~is_fixed[:, None]  # shape: (N, 1)\n",
    "    # (1) We have gradient directions grad = g_k = ∇f(x_k)\n",
    "    # (2) Compute Polak-Ribiere parameter β_k (make history grad have bigger ratio if gradient differ too much)\n",
    "    beta = np.sum(grad * (grad - g_prev), axis=1) / (np.sum(g_prev**2, axis=1) + 1e-10)\n",
    "    beta = np.clip(beta, 0, None)  # make sure β_k >= 0\n",
    "    # (3) Compute conjugate directions d = -grad + beta*d_prev\n",
    "    dir = -grad + beta[:, None] * d_prev # d_prev.shape = (N, 2)\n",
    "    # (4) Compute step size alpha = s/norm(d)\n",
    "    alpha = step_size / (np.linalg.norm(dir, axis=1) + 1e-10) # small d --> big step, big d --> small step (improve convergence)\n",
    "\n",
    "    # (RL-0) Prepare each cell's state (N, D)\n",
    "    extra_feats = np.hstack([alpha[:, None], beta[:, None], dir])  # shape (N, 4)   ps. dir.shape = (N, 2)\n",
    "    obs_scalar = np.hstack([obs[\"scalar\"], extra_feats])          # shape (N, D+4)\n",
    "    state_tensor = torch.FloatTensor(obs_scalar)\n",
    "    # (RL-1) Use PPO policy to predict (α, β, λ)\n",
    "    with torch.no_grad():\n",
    "        alpha_RL, beta_RL, lambda_RL, logprob = ppo_policy(state_tensor)\n",
    "    # (RL-2) Update α, β, and grad use RL's (α, β, λ)\n",
    "    alpha_RL = alpha_RL.squeeze().numpy()\n",
    "    alpha_RL = np.where(np.isnan(alpha_RL), alpha, alpha_RL)  # shape: (N,)\n",
    "    beta_RL = beta_RL.squeeze().numpy()\n",
    "    beta_RL = np.where(np.isnan(beta_RL), beta, beta_RL)\n",
    "    lambda_RL = lambda_RL.squeeze().numpy()\n",
    "    lambda_RL = np.where(np.isnan(lambda_RL), 1.0, lambda_RL)\n",
    "    logprob = logprob.squeeze().numpy()\n",
    "    # (RL-3) Prepare to store experience\n",
    "    # experience = (obs_scalar, np.stack([alpha_RL, beta_RL, lambda_RL.squeeze()], axis=1), logprob) # (state, action, logprob)\n",
    "    movable_mask_flat = movable_mask.squeeze()\n",
    "    experience = (\n",
    "        obs_scalar[movable_mask_flat],\n",
    "        np.stack([alpha_RL, beta_RL, lambda_RL.squeeze()], axis=1)[movable_mask_flat],\n",
    "        logprob[movable_mask_flat]\n",
    "    )\n",
    "    # (5) Update (α, Sβ) and grad\n",
    "    alpha_new = alpha + alpha_RL\n",
    "    beta_new = beta + beta_RL\n",
    "    lambda_RL = lambda_RL[:, None]  # reshape to (N, 1)\n",
    "    grad_RL = obs[\"gWL\"] + lambda_RL * obs[\"gPot\"]  # re-compute gradients with new customized λ\n",
    "    grad_RL = adjust_force(grad_RL)\n",
    "    grads_new = (grad + grad_RL) / 2\n",
    "    dir_new = -grads_new + beta_new[:, None] * d_prev   \n",
    "    # (5) update positions: x = x_prev + alpha*d\n",
    "    cells_pos += (alpha_new[:, None] * dir_new) * movable_mask\n",
    "\n",
    "    # Test: save alpha, beta, lambda for visualization\n",
    "    save_alpha_beta_lambda(alpha, beta, obs[\"weight_density\"], \"pr\")\n",
    "    save_alpha_beta_lambda(alpha_RL, beta_RL, lambda_RL, \"rl\")\n",
    "    save_alpha_beta_lambda(alpha_new, beta_new, lambda_RL, \"combined\")\n",
    "\n",
    "    return cells_pos, dir_new, experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PPO policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main training process of analytical placement function with RL-assisted CG ## \n",
    "EPOCH = 2 # Train PPO policy EPOCH times\n",
    "K = 10 # Train PPO policy every K iterations\n",
    "# Initialize our PPO policy\n",
    "ppo_policy = RLPolicy()\n",
    "ppo_optimizer = torch.optim.Adam(ppo_policy.parameters(), lr=1e-3)\n",
    "buffer = PPOBuffer()\n",
    "alpha_hist= {\"pr\": [], \"rl\": [], \"combined\": []}; beta_hist= {\"pr\": [], \"rl\": [], \"combined\": []}; lambda_hist= {\"pr\": [], \"rl\": [], \"combined\": []}\n",
    "print(\"=== PPO Training ===\")\n",
    "for epoch in range(EPOCH):\n",
    "    # Initialize cells positions and parameters\n",
    "    netlist.cells_pos = initialize_placement(len(netlist.cells_name), board_size, netlist.cells_isfixed)\n",
    "    # netlist.cells_pos = bound_cells(netlist.cells_pos)\n",
    "    # Initialize bins expected potential\n",
    "    init_density_model()\n",
    "    update_potential_base()\n",
    "    smooth_base_potential()\n",
    "    update_exp_bin_potential(show_info=True)\n",
    "\n",
    "    num_iter = ITER\n",
    "    gamma = GAMMA\n",
    "    step_size = board_size / 32\n",
    "    weight_density = None\n",
    "    DDW_TYPE = 2 # Dynamic Density Weight STRATEGY, 0: gWL/gD, 1: NTUplace, 2: RePlAce\n",
    "\n",
    "    g_prev = np.zeros_like(netlist.cells_pos)  # init grad = 0\n",
    "    d_prev = np.zeros_like(netlist.cells_pos)  # init dir = 0\n",
    "    hpwl_prev = 0\n",
    "    early_stop = False\n",
    "    iter_after_early_stop = 0\n",
    "    enough_iter = 15\n",
    "    total_over_den_prev = 0\n",
    "    max_den_prev = 0\n",
    "    total_obj_value_prev = 0\n",
    "    max_backtrak = 4 # 10\n",
    "    isMaxPhiCoefChanged = False\n",
    "    # Iteratively optimize placement\n",
    "    for iter in range(num_iter):\n",
    "        # Calculate wirelength\n",
    "        total_hpwl = calculate_hpwl(netlist.cells_pos, netlist.nets)\n",
    "        total_lse = calculate_lse_wl(netlist.cells_pos, netlist.nets, gamma=gamma)\n",
    "        # Calculate Density\n",
    "        compute_new_potential_grid()\n",
    "        update_potential_grid()\n",
    "        total_density_overflow, density_overflow_map, max_den = calculate_overflow_density()\n",
    "        total_potential_overflow, potential_overflow_map, max_pot = calculate_overflow_potential()\n",
    "        # Calculate gradient\n",
    "        gradient, lambda_new, gWL, gPot = calculate_objective_gradient(lambda_prev=weight_density, hpwl=total_hpwl, hpwl_prev=hpwl_prev) # actually here is calculate_potentail_gradient\n",
    "        weight_density = lambda_new\n",
    "        # Prepare each cell's state \n",
    "        # obs = generate_observation(gWL, gPot, weight_density)\n",
    "        obs_scalar = np.hstack([\n",
    "            gWL,                     # shape (N, 2)\n",
    "            gPot,                    # shape (N, 2)\n",
    "            np.full((gWL.shape[0], 1), weight_density)  # shape (N, 1)\n",
    "        ])\n",
    "        obs = {\"scalar\": obs_scalar, \"gWL\": gWL, \"gPot\": gPot, \"weight_density\": np.full((gWL.shape[0], 1), weight_density)}\n",
    "        # Update positions\n",
    "        netlist.cells_pos, dk, experience = update_positions_rlcg(netlist.cells_pos, g_prev, d_prev, gradient, ppo_policy, obs, step_size=step_size, is_fixed=netlist.cells_isfixed)\n",
    "        netlist.cells_pos = bound_cells(netlist.cells_pos) # bound cells to board size\n",
    "        # Calculate objective value\n",
    "        total_obj_value = calculate_obj_value(total_lse, total_potential_overflow, weight_density)\n",
    "        total_cost = calculate_cost(total_hpwl, total_density_overflow, weight_density)\n",
    "        # Collect transition\n",
    "        reward = compute_reward(total_obj_value, total_obj_value_prev)\n",
    "        reward_vec = np.full((len(experience[0]),), reward, dtype=np.float32)\n",
    "        collect_transition(buffer, experience[0], experience[1], reward_vec, experience[2])\n",
    "        # Train PPO policy every K iterations\n",
    "        if (iter > 0 and iter % K == 0) or (early_stop==True and iter_after_early_stop<=0):\n",
    "            loss_pi = train_ppo(ppo_policy, ppo_optimizer, buffer)\n",
    "            buffer.clear()\n",
    "            print(f\"[Epoch {epoch}] - [Iter {iter}] Loss: {loss_pi.item():.6f}\")\n",
    "\n",
    "        # Check early stop (if want to stop, run more 10% iterations)\n",
    "        enoughIter = iter > enough_iter\n",
    "        spreadEnough = total_density_overflow < target_density + 0.2\n",
    "        increaseOverDen = total_potential_overflow > total_over_den_prev # total_density_overflow > total_over_den_prev\n",
    "        increaseMaxDen = max_den > max_den_prev\n",
    "        notEfficientOptimize = 0.5 * total_potential_overflow * weight_density / total_obj_value * 100.0 > 95\n",
    "        if enoughIter and notEfficientOptimize:\n",
    "            print(f'Failed to further optimize at iteration {iter}')\n",
    "            early_stop = True; \n",
    "            iter_after_early_stop = int(iter*0.1) if iter_after_early_stop==0 else iter_after_early_stop\n",
    "        if enoughIter and spreadEnough and increaseOverDen and increaseMaxDen:\n",
    "            print(f'Cannot further reduce over density at iteration {iter}')\n",
    "            early_stop = True; \n",
    "            iter_after_early_stop = int(iter*0.1) if iter_after_early_stop==0 else iter_after_early_stop\n",
    "        # update last values\n",
    "        g_prev = gradient.copy(); d_prev = dk.copy()\n",
    "        total_over_den_prev = total_potential_overflow #total_density_overflow\n",
    "        max_den_prev = max_den\n",
    "        hpwl_prev = total_hpwl\n",
    "        total_obj_value_prev = total_obj_value\n",
    "        if isMaxPhiCoefChanged and ((iter) % max_backtrak) == 0:\n",
    "            isMaxPhiCoefChanged = False\n",
    "        if isMaxPhiCoefChanged and ((iter+1) % max_backtrak) == 0:\n",
    "            isMaxPhiCoefChanged = True\n",
    "            MAX_PhiCoef *= 0.99\n",
    "\n",
    "        if early_stop:\n",
    "            iter_after_early_stop -= 1\n",
    "        if early_stop and (iter_after_early_stop<=0):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== plot α, β, λ trend（前 6 個 cell） ==========\n",
    "fig, axs = plt.subplots(3, 6, figsize=(18, 6))\n",
    "keys = [\"Polak-Ribiere\", \"RL\", \"Combined\"]\n",
    "for i in range(6):\n",
    "    axs[0][i].plot([v[i] for v in alpha_hist[\"pr\"]], label=keys[0])\n",
    "    axs[0][i].plot([v[i] for v in alpha_hist[\"rl\"]], label=keys[1])\n",
    "    axs[0][i].plot([v[i] for v in alpha_hist[\"combined\"]], label=keys[2])\n",
    "    axs[0][i].set_title(f\"Cell {i}\")\n",
    "    axs[0][i].set_ylabel(\"α\")\n",
    "    axs[0][i].legend()\n",
    "\n",
    "    axs[1][i].plot([v[i] for v in beta_hist[\"pr\"]], label=keys[0])\n",
    "    axs[1][i].plot([v[i] for v in beta_hist[\"rl\"]], label=keys[1])\n",
    "    axs[1][i].plot([v[i] for v in beta_hist[\"combined\"]], label=keys[2])\n",
    "    axs[1][i].set_ylabel(\"β\")\n",
    "    axs[1][i].legend()\n",
    "\n",
    "    axs[2][i].plot([v[i] for v in lambda_hist[\"pr\"]], label=keys[0])\n",
    "    axs[2][i].plot([v[i] for v in lambda_hist[\"rl\"]], label=keys[1])\n",
    "    axs[2][i].plot([v[i] for v in lambda_hist[\"combined\"]], label=keys[2])\n",
    "    axs[2][i].set_ylabel(\"λ\")\n",
    "    axs[2][i].legend()\n",
    "\n",
    "for i in range(6):\n",
    "    axs[2][i].set_xlabel(\"Iteration\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test PPO policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main analytical placement function with RL-assisted CG ## \n",
    "# Initialize cells positions and parameters\n",
    "netlist.cells_pos = init_pos.copy()\n",
    "num_iter = 20\n",
    "gamma = GAMMA\n",
    "step_size = board_size / 32\n",
    "weight_density = None\n",
    "DDW_TYPE = 2 # Dynamic Density Weight STRATEGY, 0: gWL/gD, 1: NTUplace, 2: RePlAce\n",
    "EPOCH = 100 # Train PPO policy EPOCH times\n",
    "K = 10 # Train PPO policy every K iterations\n",
    "\n",
    "g_prev = np.zeros_like(netlist.cells_pos)  # init grad = 0\n",
    "d_prev = np.zeros_like(netlist.cells_pos)  # init dir = 0\n",
    "hpwl_prev = 0\n",
    "early_stop = False\n",
    "enough_iter = 15\n",
    "total_over_den_prev = 0\n",
    "max_den_prev = 0\n",
    "total_obj_value_prev = 0\n",
    "max_backtrak = 4 # 10\n",
    "isMaxPhiCoefChanged = False\n",
    "# Iteratively optimize placement  \n",
    "for iter in range(num_iter):\n",
    "    # Calculate wirelength\n",
    "    total_hpwl = calculate_hpwl(netlist.cells_pos, netlist.nets)\n",
    "    total_lse = calculate_lse_wl(netlist.cells_pos, netlist.nets, gamma=gamma)\n",
    "    # Calculate Density\n",
    "    compute_new_potential_grid()\n",
    "    update_potential_grid()\n",
    "    total_density_overflow, density_overflow_map, max_den = calculate_overflow_density()\n",
    "    total_potential_overflow, potential_overflow_map, max_pot = calculate_overflow_potential()\n",
    "    # Calculate gradient\n",
    "    gradient, lambda_new, gWL, gPot = calculate_objective_gradient(lambda_prev=weight_density, hpwl=total_hpwl, hpwl_prev=hpwl_prev) # actually here is calculate_potentail_gradient\n",
    "    weight_density = lambda_new\n",
    "    # Prepare each cell's state \n",
    "    # obs = generate_observation(gWL, gPot, weight_density)\n",
    "    obs_scalar = np.hstack([\n",
    "        gWL,                     # shape (N, 2)\n",
    "        gPot,                    # shape (N, 2)\n",
    "        np.full((gWL.shape[0], 1), weight_density)  # shape (N, 1)\n",
    "    ])\n",
    "    obs = {\"scalar\": obs_scalar, \"gWL\": gWL, \"gPot\": gPot}\n",
    "    # Update positions\n",
    "    netlist.cells_pos, dk, experience = update_positions_rlcg(netlist.cells_pos, g_prev, d_prev, gradient, ppo_policy, obs, step_size=step_size, is_fixed=netlist.cells_isfixed)\n",
    "    netlist.cells_pos = bound_cells(netlist.cells_pos) # bound cells to board size\n",
    "    # Calculate objective value\n",
    "    total_obj_value = calculate_obj_value(total_lse, total_potential_overflow, weight_density)\n",
    "    total_cost = calculate_cost(total_hpwl, total_density_overflow, weight_density)\n",
    "\n",
    "    # Check early stop\n",
    "    enoughIter = iter > enough_iter\n",
    "    spreadEnough = total_density_overflow < target_density + 0.2\n",
    "    increaseOverDen = total_potential_overflow > total_over_den_prev # total_density_overflow > total_over_den_prev\n",
    "    increaseMaxDen = max_den > max_den_prev\n",
    "    notEfficientOptimize = 0.5 * total_potential_overflow * weight_density / total_obj_value * 100.0 > 95\n",
    "    if enoughIter and notEfficientOptimize:\n",
    "        print(f'Failed to further optimize at iteration {iter}')\n",
    "        early_stop = True\n",
    "    if enoughIter and spreadEnough and increaseOverDen and increaseMaxDen:\n",
    "        print(f'Cannot further reduce over density at iteration {iter}')\n",
    "        early_stop = True\n",
    "    # update last values\n",
    "    g_prev = gradient.copy(); d_prev = dk.copy()\n",
    "    total_over_den_prev = total_potential_overflow #total_density_overflow\n",
    "    max_den_prev = max_den\n",
    "    hpwl_prev = total_hpwl\n",
    "    total_obj_value_prev = total_obj_value\n",
    "    if isMaxPhiCoefChanged and ((iter) % max_backtrak) == 0:\n",
    "        isMaxPhiCoefChanged = False\n",
    "    if isMaxPhiCoefChanged and ((iter+1) % max_backtrak) == 0:\n",
    "        isMaxPhiCoefChanged = True\n",
    "        MAX_PhiCoef *= 0.99\n",
    "    \n",
    "\n",
    "    # Plot placement\n",
    "    costs_info = {\"total_hpwl\": total_hpwl, \"total_lse\": total_lse, \"total_density_overflow\": total_density_overflow, \"total_potential_overflow\": total_potential_overflow, \"total_obj_value\": total_obj_value, \"total_cost\": total_cost, \"weight_density\": weight_density, \"gWL\": np.sum(np.abs(gWL)), \"gPot\": np.sum(np.abs(gPot))}\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5), facecolor='white')\n",
    "    plot_placement(axs[0], board_size, netlist, iter, costs_info)\n",
    "    plot_density_map(fig, axs[1], board_size, bins_potential) # plot_density_map(axs[1], board_size, bins_density)\n",
    "    display(fig)\n",
    "    clear_output(wait=True)\n",
    "    plt.close(fig)  \n",
    "    plt.pause(0.001)\n",
    "\n",
    "    if early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Case For NTUplace3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "## Write bookshelf\n",
    "def write_aux(design_name, folder):\n",
    "    with open(f'{folder}/{design_name}.aux', 'w') as fp:\n",
    "        fp.write(f\"RowBasedPlacement :  {design_name}.nodes  {design_name}.nets  {design_name}.wts  {design_name}.pl  {design_name}.scl\\n\")\n",
    "def write_nodes(design_name, folder, date=None):\n",
    "    with open(f'{folder}/{design_name}.nodes', 'w') as fp:\n",
    "        fp.write(\"UCLA nodes 1.0\\n\")\n",
    "        fp.write(f\"# Created      : {date}\\n\")\n",
    "        fp.write(\"# User         : frankchiang@eda.ee.ntu.edu.tw (Cheng-Yu Chiang)\\n\")\n",
    "        fp.write(\"# Institution  : EDA Lab, NTU\\n\\n\")\n",
    "        fp.write(f\"NumNodes : {len(netlist.cells_name)}\\n\")\n",
    "        fp.write(f\"NumTerminals : {np.sum(netlist.cells_isfixed)}\\n\")\n",
    "        for i in range(len(netlist.cells_name)):\n",
    "            fp.write(f\"{netlist.cells_name[i]}    {netlist.cells_size[i][0]}    {netlist.cells_size[i][1]}\" + (\"    terminal\" if netlist.cells_isfixed[i]==1 else \"\") + \"\\n\")\n",
    "def write_pl(design_name, folder, date=None):\n",
    "    with open(f'{folder}/{design_name}.pl', 'w') as fp:\n",
    "        fp.write(\"UCLA pl 1.0\\n\")\n",
    "        fp.write(f\"# Created      : {date}\\n\")\n",
    "        fp.write(\"# User         : frankchiang@eda.ee.ntu.edu.tw (Cheng-Yu Chiang)\\n\")\n",
    "        fp.write(\"# Institution  : EDA Lab, NTU\\n\\n\")\n",
    "        for i in range(len(netlist.cells_name)):\n",
    "            fp.write(f\"{netlist.cells_name[i]}    {int(init_pos[i][0]-netlist.cells_size[i][0]/2)}    {int(init_pos[i][1]-netlist.cells_size[i][1]/2)}\" + (\"    : N\" if netlist.cells_isfixed[i]==0 else \"    : N /FIXED\") + \"\\n\")\n",
    "def write_nets(design_name, folder, date=None):\n",
    "    with open(f'{folder}/{design_name}.nets', 'w') as fp:\n",
    "        fp.write(\"UCLA pl 1.0\\n\")\n",
    "        fp.write(f\"# Created      : {date}\\n\")\n",
    "        fp.write(\"# User         : frankchiang@eda.ee.ntu.edu.tw (Cheng-Yu Chiang)\\n\")\n",
    "        fp.write(\"# Institution  : EDA Lab, NTU\\n\\n\")\n",
    "        fp.write(f\"NumNets : {len(netlist.nets)}\\n\")\n",
    "        fp.write(f\"NumPins : {sum(len(net) for net in netlist.nets)}\\n\")\n",
    "        for i in range(len(netlist.nets)):\n",
    "            fp.write(f\"NetDegree : {len(netlist.nets[i])}\\n\")\n",
    "            for j in range(len(netlist.nets[i])):\n",
    "                fp.write(f\"    {netlist.cells_name[netlist.nets[i][j]]} I : 0    0\\n\")\n",
    "def write_wts(design_name, folder, date=None):\n",
    "    with open(f'{folder}/{design_name}.wts', 'w') as fp:\n",
    "        fp.write(\"UCLA pl 1.0\\n\")\n",
    "        fp.write(f\"# Created      : {date}\\n\")\n",
    "        fp.write(\"# User         : frankchiang@eda.ee.ntu.edu.tw (Cheng-Yu Chiang)\\n\")\n",
    "        fp.write(\"# Institution  : EDA Lab, NTU\\n\\n\")\n",
    "def write_scl(design_name, folder, date=None):\n",
    "    with open(f'{folder}/{design_name}.scl', 'w') as fp:\n",
    "        fp.write(\"UCLA pl 1.0\\n\")\n",
    "        fp.write(f\"# Created      : {date}\\n\")\n",
    "        fp.write(\"# User         : frankchiang@eda.ee.ntu.edu.tw (Cheng-Yu Chiang)\\n\")\n",
    "        fp.write(\"# Institution  : EDA Lab, NTU\\n\\n\")\n",
    "        fp.write(f\"NumRows : {board_size}\\n\")\n",
    "        fp.write(\"\\n\")\n",
    "        for i in range(board_size):\n",
    "            fp.write(f\"CoreRow Horizontal\\n\")\n",
    "            fp.write(f\"  Coordinate    :   {i}\\n\")\n",
    "            fp.write(f\"  Height        :   {1}\\n\")\n",
    "            fp.write(f\"  Sitewidth     :   {1}\\n\")\n",
    "            fp.write(f\"  Sitespacing   :   {1}\\n\")\n",
    "            fp.write(f\"  Siteorient    :   {0}\\n\")\n",
    "            fp.write(f\"  Sitesymmetry  :   {1}\\n\")\n",
    "            fp.write(f\"  SubrowOrigin  :   {0}    NumSites  :  {board_size}\\n\")\n",
    "            fp.write(f\"END\\n\")\n",
    "def write_bookshelf(design_name, folder, date=None):\n",
    "    os.system(f\"rm -rf {folder} && mkdir -p {folder}\")\n",
    "    write_aux(design_name, folder)\n",
    "    write_nodes(design_name, folder, date=date)\n",
    "    write_pl(design_name, folder, date=date)\n",
    "    write_nets(design_name, folder, date=date)\n",
    "    write_wts(design_name, folder, date=date)\n",
    "    write_scl(design_name, folder, date=date)\n",
    "    print(f\"Write {folder}/{design_name}.aux, {design_name}.nodes, {design_name}.pl, {design_name}.nets, {design_name}.wts, {design_name}.scl\")\n",
    "\n",
    "write_bookshelf(\"toy0\", \"../../benchmarks/toy\", date=datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntuplace3re",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
